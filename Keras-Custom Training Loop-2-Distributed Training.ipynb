{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983a6f8a",
   "metadata": {},
   "source": [
    "# Custom Training Loop using TensorFlow Keras: Distributed Training\n",
    "\n",
    "In this notebook, we describe the steps for creating a custom training loop using TensorFlow Keras for **distributed training**. Specifically, we use the **synchronous data parallelism** technique, where a single model gets replicated on multiple devices or multiple machines (e.g., multiple GPUs). Each of the devices processes different batches of data, then they merge their results. Thus, different replicas of the model stay in sync after each batch they process. Synchronicity keeps the model convergence behavior identical to what we would see for single-device training.\n",
    "\n",
    "### Single-Host, Multi-Device Training\n",
    "For implementing the synchronous data parallelism-based distributed training, we use the tf.distribute.MirroredStrategy API to train TensorFlow Keras models on multiple GPUs installed on a single machine.\n",
    "\n",
    "    -- The specification for the number and type of GPUs should be provided in the SLURM .sh job request file. \n",
    "\n",
    "\n",
    "### How does the tf.distribute.MirroredStrategy Strategy Work?\n",
    "\n",
    "- All the variables and the model graph are replicated across the replicas (e.g., GPUs).\n",
    "- Input data is evenly distributed across the replicas.\n",
    "- Each replica calculates the loss and gradients for the input it received.\n",
    "- The gradients are synced across all the replicas by summing them.\n",
    "- After the sync, the same update is made to the copies of the variables on each replica.\n",
    "\n",
    "### A Summary of the Steps for Single-host, Multi-device Synchronous Training\n",
    "\n",
    "- Instantiate a MirroredStrategy. By default, the strategy will use all GPUs available on a single machine.\n",
    "\n",
    "- Create a tf.data.Dataset which is required to load data in a multi-device or distributed workflow. Use tf.distribute.Strategy.experimental_distribute_dataset to convert the tf.data.Dataset to something that produces \"per-replica\" values. \n",
    "\n",
    "- Use the strategy object to open a scope, and within this scope, create all the Keras objects that contain variables. These include the model, optimizer, checkpoint, loss functions, and metrics. \n",
    "\n",
    "- Define functions to train and validate the model using a single batch of data. Then, use the tf.distribute.Strategy.run method of the mirrored strategy to run these functions once per replica, taking \"per-replica\" values (e.g., from a tf.distribute.DistributedDataset object) and returning \"per-replica\" values.  The \"run\" method is executed in the \"replica context\", which means each operation is performed separately on each replica.\n",
    "\n",
    "- Finally use a method (such as tf.distribute.Strategy.reduce) to convert the resulting \"per-replica\" values into ordinary Tensors.\n",
    "\n",
    "\n",
    "More information on distributed training: https://keras.io/guides/distributed_training/\n",
    "\n",
    "More information on distributed training for custom training loop: https://www.tensorflow.org/tutorials/distribute/custom_training\n",
    "\n",
    "\n",
    "\n",
    "## Summary of the Other Techniques/Tools Used in this Notebook\n",
    "\n",
    "In addition to describing the design of a custom training loop for distributed training, we use the following techniques/tools that are useful in practical deep learning tasks.\n",
    "\n",
    "- Store and pre-process the data in the TensorFlow Dataset format.\n",
    "\n",
    "- Build a model by defining a custom layer.\n",
    "\n",
    "- Utilize various optimizers and schedulers.\n",
    "\n",
    "- Model serialization.\n",
    "\n",
    "        -- Serialize the final model (with custom layers) in TensorFlow's SavedModel format. \n",
    "        -- Serialize the intermediate model checkpoints (only the parameters of the model) using the tf.train.Checkpoint class. Customize the checkpoint object.\n",
    "\n",
    "- Loading the saved model.\n",
    "        \n",
    "        -- Final SavedModel\n",
    "        -- Intermediate model checkpoints\n",
    "\n",
    "- Monitor the Training Process\n",
    "\n",
    "        -- Use comet ml for monitoring the training in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a934bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml at the top of your file.\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key.\n",
    "experiment = Experiment(\n",
    "    api_key=\"\",\n",
    "    project_name=\"\",\n",
    "    workspace=\"\",\n",
    "    auto_histogram_weight_logging=True,\n",
    "    auto_histogram_gradient_logging=True,\n",
    "    auto_histogram_activation_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python libraries.\n",
    "import os\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch # torch is used to get GPU information.\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa # required for some optimizers, e.g., AdamW, LAMB.\n",
    "\n",
    "# Display the TensorFlow version.\n",
    "print(\"\\nTensorFlow Version: \" , tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to store the number of available GPUs.\n",
    "num_of_gpu = 0\n",
    "\n",
    "# Determine the number of GPUs available.\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell torch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Get the number of GPUs.\n",
    "    num_of_gpu = torch.cuda.device_count()\n",
    "    print(\"Number of available GPU(s) %d.\" % num_of_gpu)\n",
    "\n",
    "    print(\"GPU Name: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a MirroredStrategy.\n",
    "'''\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"\\nNumber of GPU(s): {}\".format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aca8c4",
   "metadata": {},
   "source": [
    "## Load & Scale the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the dataset (training & test).\n",
    "'''\n",
    "(X_train_all, y_train_all), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "# Convert datatype to float32.\n",
    "X_train_all = X_train_all.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Create a validation subset.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.15, random_state=42)\n",
    "\n",
    "# Scale the data.\n",
    "X_train = X_train/255.0\n",
    "X_val = X_val/255.0\n",
    "X_test = X_test/255.0\n",
    "\n",
    "print(\"No. of Training Samples: \", X_train.shape)\n",
    "print(\"No. of Training Labels: \", y_train.shape)\n",
    "\n",
    "print(\"\\nNo. of Validation Samples: \", X_val.shape)\n",
    "print(\"No. of Validation Labels: \", y_val.shape)\n",
    "\n",
    "print(\"\\nNo. of Testing Samples: \", X_test.shape)\n",
    "print(\"No. of Testing Labels: \", y_test.shape)\n",
    "\n",
    "print(\"\\nX type: \", X_train.dtype)\n",
    "print(\"y type: \", y_train.dtype)\n",
    "\n",
    "\n",
    "# Create one-hot encoded labels.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"\\ny train (shape): \", y_train.shape)\n",
    "print(\"y val (shape): \", y_val.shape)\n",
    "print(\"y test (shape): \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8b20e",
   "metadata": {},
   "source": [
    "## Create TensorFlow Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n########################## Create TensorFlow Dataset objects ##########################\\n\\n\")\n",
    "\n",
    "'''\n",
    "For loading the data dynamically and pre-processing (if required), \n",
    "we use TensorFlow's Data API (tf.data).\n",
    "Specifically, from the NumPy arrays (feature & label), \n",
    "we construct TensorFlow Dataset (TensorSliceDataset) objects.\n",
    "'''\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "train_dataset_size = len(list(train_dataset.as_numpy_iterator()))\n",
    "print(\"\\nTrain Dataset (shape of a single sample and its type):\\n\", train_dataset)\n",
    "print(\"Train Dataset (size): \", train_dataset_size)\n",
    "\n",
    "val_dataset_size = len(list(val_dataset.as_numpy_iterator()))\n",
    "print(\"\\nValidation Dataset (shape of a single sample and its type):\\n\", val_dataset)\n",
    "print(\"Validation Dataset (size): \", val_dataset_size)\n",
    "\n",
    "test_dataset_size = len(list(test_dataset.as_numpy_iterator()))\n",
    "print(\"\\nTest Dataset (shape of a single sample and its type):\\n\", test_dataset)\n",
    "print(\"Test Dataset (size): \", test_dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab718",
   "metadata": {},
   "source": [
    "## Dataset Object Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to shuffle & batch the elements of the Dataset object.\n",
    "\n",
    "- The shuffle method randomly shuffles the elements of the Dataset object. \n",
    "First, it fills a buffer with the Dataset with buffer_size elements. \n",
    "Then, it randomly samples elements from this buffer, replacing the selected elements with new elements. \n",
    "For perfect shuffling, the buffer_size should be greater than or equal to the size of the Dataset. \n",
    "However, for large Dataset objects, this isn't possible. So, we will use a large enough buffer_size.\n",
    "\n",
    "\n",
    "- In the batch method, we set \"drop_remainder\" to True so that the size of the training set is \n",
    "   divisible by the batch_size. It is done by removing enough training examples.\n",
    "\n",
    "'''\n",
    "def prepare_dataset(ds, mini_batch, repeat=1, shuffle=False, buffer_size=0):\n",
    "    '''\n",
    "    Cache the Dataset elements in memory\n",
    "    '''\n",
    "    ds = ds.cache()\n",
    "  \n",
    "    '''\n",
    "    Shuffle the elements of Dataset\n",
    "    '''\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size)\n",
    "      \n",
    "    '''\n",
    "    Repeat the elements of the shuffled Dataset\n",
    "    '''  \n",
    "    ds = ds.repeat(count=repeat)\n",
    "\n",
    "    '''\n",
    "    Batch the elements of the Dataset\n",
    "    '''\n",
    "    ds = ds.batch(mini_batch, drop_remainder=True)\n",
    "    \n",
    "    '''\n",
    "    Use buffered prefecting on all elements of the Dataset\n",
    "    '''\n",
    "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n########################## Dataset Object Pre-Processing ##########################\\n\\n\")\n",
    "\n",
    "'''\n",
    "Determine the GLOBAL_BATCH_SIZE (the total size of mini-batch for all GPUs) for training.\n",
    "It should be a multiple of BATCH_SIZE_PER_REPLICA .\n",
    "The multiplication factor is determined by the number of available GPUs (or strategy.num_replicas_in_sync)\n",
    "'''\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 128\n",
    "\n",
    "if(num_of_gpu > 0):\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA*num_of_gpu\n",
    "else:\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA # Uses the CPU, as no GPU is available\n",
    "\n",
    "\n",
    "'''\n",
    "Size of the validation mini-batch. \n",
    "'''\n",
    "size_of_mini_batch_val = GLOBAL_BATCH_SIZE\n",
    "\n",
    "print(\"\\nSize of training mini-batch: \", GLOBAL_BATCH_SIZE)\n",
    "print(\"Size of validation mini-batch: \", size_of_mini_batch_val)\n",
    "\n",
    "\n",
    "'''\n",
    "The \"buffer_size\" variable is used by the \"shuffle\" method. \n",
    "For a small Dataset object, it should be equal to or larger than training set.\n",
    "'''\n",
    "buffer_size = train_dataset_size\n",
    "print(\"\\nBuffer Size: \", buffer_size)\n",
    "\n",
    "'''\n",
    "Set the number of training epochs.\n",
    "This is required by the repeat method.\n",
    "'''\n",
    "no_of_epochs = 100\n",
    "print(\"\\nEpochs: \", no_of_epochs)\n",
    "\n",
    "'''\n",
    "Perform data pre-processing by the CPU.\n",
    "It is efficient to use the CPU as it ensures that the GPUs will be used only for model training.\n",
    "'''\n",
    "with tf.device('/cpu:0'):\n",
    "    train_loader = prepare_dataset(train_dataset, GLOBAL_BATCH_SIZE, repeat=1, \n",
    "                                   shuffle=True, buffer_size=buffer_size)\n",
    "    val_loader = prepare_dataset(val_dataset, size_of_mini_batch_val)\n",
    "    test_loader = prepare_dataset(test_dataset, size_of_mini_batch_val)\n",
    "    no_of_steps_per_epoch = train_dataset_size//GLOBAL_BATCH_SIZE\n",
    "    print(\"Number steps/epoch: \", no_of_steps_per_epoch)\n",
    "    total_no_of_steps = train_loader.cardinality().numpy()\n",
    "    print(\"Data available to run for %d epochs\" % (total_no_of_steps//no_of_steps_per_epoch))\n",
    "    print(\"Unlimited data available: \", (train_loader.cardinality() == tf.data.INFINITE_CARDINALITY).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90415c",
   "metadata": {},
   "source": [
    "## Distribute the Dataset Objects Over the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f75fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Distribute the Dataset objects based on the mirrored strategy by using the \n",
    "tf.distribute.Strategy.experimental_distribute_dataset method.\n",
    "\n",
    "This method converts a tf.data.Dataset to something that produces \"per-replica\" values. \n",
    "\n",
    "Alternatively, we can manually specify how the dataset should be partitioned across replicas. \n",
    "For this, we may use the tf.distribute.Strategy.distribute_datasets_from_function.\n",
    "'''\n",
    "dist_train_loader = mirrored_strategy.experimental_distribute_dataset(train_loader)\n",
    "dist_val_loader = mirrored_strategy.experimental_distribute_dataset(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd31afa",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "We build a VGGNet model by defining a custom layer (for the VGG block). \n",
    "\n",
    "- First, define a VGG block (layer) class using the subclass of the tf.keras.layers.Layer class. This custom layer uses a get_config method for serialization.\n",
    "- Then, define the model function by flexibly utilizing the VGG block class.\n",
    "\n",
    "\n",
    "### Optimizers and Schedulers\n",
    "\n",
    "We provide options for utilizing various optimizers and schedulers.\n",
    "\n",
    "- Optimizers: \n",
    "\n",
    "        -- SGD (Stochastic Gradient Descent)\n",
    "        -- Adam (Adaptive moment estimation)\n",
    "        -- Nadam (Adam with Nesterov momentum)\n",
    "        -- AdamW (a variant of Adam where the weight decay is performed only after controlling the parameter-wise step size), useful for adapting the learning rate in small batch settings\n",
    "        -- LAMB (Layer-wise Adaptive Moments optimizer for Batch training), useful for adapting the learning rate in large batch settings\n",
    "\n",
    "- Schedulers for SGD:\n",
    "\n",
    "        -- Exponential Decay\n",
    "        -- Piecewise Constant Decay\n",
    "        -- Cosine Decay Restarts\n",
    "\n",
    "##### The model, the optimizer, and the checkpoint must be created under the mirrored strategy scope.\n",
    "\n",
    "\n",
    "#### Save the Parameters (variables) of the Model Intermittently\n",
    "\n",
    "After the training is complete, we serialize the final model in TensorFlow's SavedModel format. SavedModel is a comprehensive save format that saves the model architecture, parameters (weights), and the traced Tensorflow subgraphs of the call functions. \n",
    "\n",
    "However, during the training we save **only the parameters of the model at a regular interval**, e.g., after every fixed number of epochs. These objects are called model checkpoints. For this intermittent saving (and restoring), we use the tf.train.Checkpoint class. \n",
    "\n",
    "We customize the saving process of model checkpoints by using the tf.train.CheckpointManager class.\n",
    "\n",
    "- To keep the only last few Checkpoints. E.g., we keep the last 5 checkpoints.\n",
    "- To use the epoch number for numbering the saved checkpoints. By default, checkpoints are numbered from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define the VGG block class using Keras Sequential API.\n",
    "The VGG_Block class takes two arguments:\n",
    "- conv_block_number: number of convolutional layers \n",
    "- num_of_channels: number of output channels \n",
    "'''\n",
    "class VGG_Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, conv_block_number, num_of_channels, weight_decay, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_layers = [] \n",
    "        for _ in range(conv_block_number):\n",
    "            self.conv_layers.append(tf.keras.layers.Conv2D(filters=num_of_channels, kernel_size=(3, 3), strides=1,\n",
    "                                padding='same', kernel_regularizer=tf.keras.regularizers.l2(weight_decay), use_bias=False))\n",
    "            self.conv_layers.append(tf.keras.layers.BatchNormalization())\n",
    "            self.conv_layers.append(tf.keras.layers.Activation(\"relu\"))\n",
    "        \n",
    "        self.pool_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.conv_layers:\n",
    "            Z = layer(Z)\n",
    "            \n",
    "        Z = self.pool_layer(Z)\n",
    "        return Z\n",
    "        \n",
    "    # Required for the custom object's serialization.\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"conv_layers\": self.conv_layers,\n",
    "            \"pool_layer\": self.pool_layer,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "'''\n",
    "Function to create the VGGNet model.\n",
    "'''\n",
    "def create_model(conv_blocks, width, height, channels, num_classes, weight_decay, augmentation=False, **kwargs):\n",
    "    \n",
    "    vgg_net = tf.keras.models.Sequential(name='VGG')\n",
    "    \n",
    "    vgg_net.add(tf.keras.layers.InputLayer(input_shape=(width, height, channels)))\n",
    "    \n",
    "    # Data augmentation layer.\n",
    "    if(augmentation):\n",
    "        vgg_net.add(data_augmentation_layer(**kwargs))\n",
    "    \n",
    "    # Convolutional layers based on the VGG_Block object.\n",
    "    for (conv_block_number, num_of_channels) in conv_blocks:\n",
    "            vgg_net.add(VGG_Block(conv_block_number, num_of_channels, weight_decay))\n",
    "    \n",
    "    # Flatten the convnet output to feed it with fully-connected layers.\n",
    "    vgg_net.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # Fully-connected layers\n",
    "    vgg_net.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "    vgg_net.add(tf.keras.layers.Dropout(0.5))\n",
    "    vgg_net.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "    \n",
    "    return vgg_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n########################## Create the Model ##########################\\n\\n\")\n",
    "\n",
    "'''\n",
    "Reset all state generated by Keras.\n",
    "It deletes the TensorFlow graph before creating a new model, \n",
    "otherwise memory overflow will occur.\n",
    "'''\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "'''\n",
    "To reproduce the same result by the model in each iteration, we use fixed seeds for random number generation. \n",
    "'''\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "'''\n",
    "Create the model and optimizer inside the strategy's scope. \n",
    "This ensures that any variables created with the model and optimizer are mirrored variables.\n",
    "'''\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    '''\n",
    "    Instantiate the model.\n",
    "    '''\n",
    "    layer_info = ((2, 64), (2, 128), (4, 256), (4, 512), (4, 512)) # for the VGGNet model\n",
    "    model = create_model(layer_info, 32, 32, channels=3, num_classes=10,\n",
    "                           weight_decay=0.001)\n",
    "    \n",
    "    '''\n",
    "    Display a summary of the model architecture.\n",
    "    '''\n",
    "    model.summary()\n",
    "    \n",
    "    '''\n",
    "    Instantiate a learning rate scheduler for the SGD optimizer.\n",
    "    There are 3 choices. \n",
    "    - Exponential decay\n",
    "    - Piecewise constant decay\n",
    "    - Cosine decay with restarts\n",
    "\n",
    "    NOTE: Uncomment only one scheduler if the SGD optimizer is used.\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    The initial learning rate is used by the optimizers, e.g., SGD, ADAM, NADAM, etc.\n",
    "    Some SGD schedulers also require an initial learning rate (e.g., exponential decay, cosine decay).\n",
    "    '''\n",
    "    initial_learning_rate=0.01\n",
    "\n",
    "    '''\n",
    "    Scheduler: ExponentialDecay\n",
    "    '''\n",
    "    # decay_steps=no_of_steps_per_epoch * 50\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #     initial_learning_rate=initial_learning_rate,\n",
    "    #     decay_steps=decay_steps,\n",
    "    #     decay_rate=0.1,\n",
    "    #     staircase=True)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Scheduler: PiecewiseConstantDecay\n",
    "    '''\n",
    "    # boundaries = [no_of_steps_per_epoch * 150, no_of_steps_per_epoch*250]\n",
    "    # values = [0.5, 0.1, 0.01]\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "    '''\n",
    "    Scheduler: CosineDecayRestarts\n",
    "    '''\n",
    "    first_decay_steps = no_of_steps_per_epoch * 20\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate,\n",
    "                                                                    first_decay_steps)\n",
    "\n",
    "    '''\n",
    "    Instantiate an optimizer. Use one of the following choices.\n",
    "    - Fixed LR: learning_rate=linitial_learning_rate\n",
    "    - Scheduled LR: learning_rate=lr_schedule\n",
    "    '''\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=False)\n",
    "    #optimizer=tf.keras.optimizers.Nadam(learning_rate=initial_learning_rate)\n",
    "    #optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "    #optimizer=tfa.optimizers.AdamW(learning_rate=initial_learning_rate, weight_decay=0.001)\n",
    "    #optimizer=tfa.optimizers.LAMB(learning_rate=initial_learning_rate, weight_decay_rate=0.001)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    The checkpoint model name variable is used for serializing model checkpoints\n",
    "    '''\n",
    "    checkpoint_model_name='Model-Distributed-Checkpoint'\n",
    "\n",
    "    '''\n",
    "    Path to the diretory in which the model checkpoints will be serialized\n",
    "    '''\n",
    "    checkpoint_model_directory='./Checkpoint_Models/'\n",
    "\n",
    "    '''\n",
    "    Instantiate the checkpoint function\n",
    "    '''\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "\n",
    "    '''\n",
    "    Instantiate the checkpoint manager function\n",
    "    '''\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_model_directory, \n",
    "                                         checkpoint_name=checkpoint_model_name, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feea874",
   "metadata": {},
   "source": [
    "## Utility Functions for Computing Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e20bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n######### Functions: Utility Functions for Loss & Accuracy ###########\\n\\n\")\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    '''\n",
    "    Define two functions to compute the training loss (per iteration/step).\n",
    "    \n",
    "    Function \"loss_object\"\n",
    "    Set reduction to `NONE` so the reduction is done afterwards and divide by global batch size.\n",
    "    The value `SUM_OVER_BATCH_SIZE` is disallowed because currently it would only divide by per replica batch size.\n",
    "    \n",
    "    Function \"compute_loss\"\n",
    "    We should sum the per example losses and divide the sum by the GLOBAL_BATCH_SIZE. \n",
    "    For this, we use tf.nn.compute_average_loss which takes the per example loss, \n",
    "    and GLOBAL_BATCH_SIZE as arguments and returns the scaled loss.\n",
    "    '''\n",
    "    loss_object=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    \n",
    "    def compute_loss(labels, predictions, model_losses):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        loss = tf.nn.compute_average_loss(per_example_loss,\n",
    "                                          global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "        \n",
    "        '''\n",
    "        If there are additional losses incurred by the model (e.g., weight regularizer loss), \n",
    "        then we should sum them up and divide the sum by the number of replicas. \n",
    "        This is accomplished by using the tf.nn.scale_regularization_loss function.\n",
    "        This function scales the sum of the given regularization losses by number of replicas.\n",
    "        '''\n",
    "        if model_losses:\n",
    "            loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    # Instantiate a function to compute mean validation loss (loss per epoch).\n",
    "    val_loss_epoch = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "    # Instantiate functions to compute train & val accuracies per epoch.   \n",
    "    train_acc_epoch = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "    val_acc_epoch = tf.keras.metrics.CategoricalAccuracy(name='val_accuracy')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087abaf",
   "metadata": {},
   "source": [
    "## Utility Functions for Displaying the Training Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n######### Functions: Utility Functions for Displaying Training Status ###########\\n\\n\")\n",
    "\n",
    "# Function to display training statistics per step \n",
    "def print_status_per_step(iteration, total, train_loss):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(\"loss\", train_loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)\n",
    "    \n",
    "# Function to display training statistics per epoch\n",
    "def print_status_per_epoch(iteration, total, train_loss, train_acc,\n",
    "                          val_loss, val_acc, lr, time_per_epoch):\n",
    "    metrics = \" - \".join([\"{}: {:.4f} - {}: {:.4f} - {}: {:.4f} - {}: {:.4f} - {}: {:.4f} {:.2f}s\"\\\n",
    "                          .format(\"loss\", train_loss,\n",
    "                                  \"acc\", train_acc,\n",
    "                                  \"val loss\", val_loss,\n",
    "                                  \"val acc\", val_acc,\n",
    "                                  \"lr\", lr,\n",
    "                                  time_per_epoch),\n",
    "                         ])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa223ddd",
   "metadata": {},
   "source": [
    "## Functions for Training & Validation of the Model For Each Batch Per-Replica\n",
    "\n",
    "We define \"per-replica\" two functions (train_step & val_step) for training and validation for each batch of data (during an epoch). \n",
    "\n",
    "        -- These functions are executed on each replica.\n",
    "\n",
    "The train_step function performs the following tasks.\n",
    "\n",
    "- Define the tf.GradientTape() block. \n",
    "\n",
    "- Inside the block, make a prediction for one batch (using the model as a function), and compute the loss. The loss consists of the main loss plus the other losses (e.g., weight regularizer loss). Note that, to save memory, we only put the strict minimum operations inside the tf.GradientTape() block. The tape is automatically erased immediately after we call its gradient() method.\n",
    "\n",
    "- Ask the tape to compute the gradient of the loss with respect to each trainable variable (by using the gradient() method).\n",
    "\n",
    "- Apply them to the optimizer to perform a Gradient Descent step ((by using the apply_gradient() method)).\n",
    "\n",
    "- Update the mean loss and the metrics (over the current epoch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions for defining train & val step functions.\n",
    "'''\n",
    "def train_step(X, y):\n",
    "        '''\n",
    "        Open a GradientTape to record the operations run\n",
    "          during the forward pass. \n",
    "        This will enable auto-differentiation for computing the loss gradient.\n",
    "\n",
    "        Run the forward pass of the layer.\n",
    "           Trainable variables are automatically tracked by GradientTape, i.e., recorded\n",
    "             on the GradientTape.\n",
    "        '''\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Predict the logits for the current minibatch.\n",
    "            y_pred = model(X, training=True) \n",
    "            \n",
    "            # Compute the loss for the current minibatch.\n",
    "            loss = compute_loss(y, y_pred, model.losses)\n",
    "            \n",
    "        '''\n",
    "        Compute the gradient of the loss function wrt the trainable variables (weights & biases).\n",
    "        Use the GradieneTape object to automatically retrieve\n",
    "          the gradients of the trainable variables with respect to the loss.\n",
    "        '''\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        '''\n",
    "        Update the weights by using the loss gradients.\n",
    "        Run one iteration/step of gradient descent by updating\n",
    "          the value of the variables to minimize the loss.\n",
    "        '''\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Update the training accuracy metric for the current epoch.\n",
    "        This method gets executed by tf.distribute.Strategy.run in the distributed_train_step function.\n",
    "        '''\n",
    "        train_acc_epoch.update_state(y, y_pred)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        \n",
    "def val_step(X_val, y_val):\n",
    "    # Predict the logits for the current validation minibatch.\n",
    "    y_val_pred = model(X_val, training=False)\n",
    "        \n",
    "    # Compute the loss for the current validation minibatch.\n",
    "    loss_val = loss_object(y_val, y_val_pred)\n",
    "    \n",
    "    '''\n",
    "    Compute the validation loss for the current epoch.\n",
    "    This method gets executed by tf.distribute.Strategy.run in the distributed_val_step function.\n",
    "    '''\n",
    "    val_loss_epoch.update_state(loss_val)\n",
    "        \n",
    "    '''\n",
    "    Compute the validation accuracy for the current epoch.\n",
    "    This method gets executed by tf.distribute.Strategy.run in the distributed_val_step function.\n",
    "    '''\n",
    "    val_acc_epoch.update_state(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798c846",
   "metadata": {},
   "source": [
    "## Functions for Distributed Training & Validation of the Model For Each Batch \n",
    "\n",
    "The key idea is to replicate the train_step and val_step functions over the GPUs and run it\n",
    "with the distributed input. This is accomplished by the mirrored strategy \"run\" method.\n",
    "The tf.distribute.Strategy.run returns results from each local replica in the strategy. \n",
    "Then, we use tf.distribute.Strategy.reduce to get an aggregated value. \n",
    "\n",
    "To compile the functions into a static graph, add a @tf.function decorator\n",
    "for both the distributed_train_step and distributed_val_step functions.\n",
    "\n",
    "Describing the computation as a static graph enables the framework to apply global performance optimizations. \n",
    "This is impossible when the framework is constrained to greedly execute one operation after another, \n",
    "with no knowledge of what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n########### Functions for Distributed Training & Validation of the Model For Each Batch #############\\n\\n\")\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(X, y):\n",
    "    per_replica_losses = mirrored_strategy.run(train_step, args=(X, y,))\n",
    "    \n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "@tf.function\n",
    "def distributed_val_step(X, y):\n",
    "    return mirrored_strategy.run(val_step, args=(X, y,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11326ede",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "The distributed_train_step and distributed_val_step functions are used to define a custom training loop.\n",
    "\n",
    "The following tasks are performed.\n",
    "\n",
    "- Create two nested loops: one for the epochs, and the other for the batches within an epoch.\n",
    "\n",
    "      -- Within the inner loop (for iterating through the batches within an epoch), call the distributed_train_step function. It will train the model on all replicas using distributed batches of data within an epoch.\n",
    "\n",
    "      -- Display the status bar to show the training statistics for each iteration and/or epoch.\n",
    "\n",
    "- The distributed_val_step function is executed within the outer loop for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n########################## Train the Model ##########################\\n\\n\")\n",
    "\n",
    "# This variable keeps a count of the total number of iterations/steps for training until end (for all epochs).\n",
    "iterations_total = 0\n",
    "\n",
    "# Perform training and validation.\n",
    "for epoch in range(1, no_of_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, no_of_epochs))\n",
    "\n",
    "    # Get the decayed learning rate per epoch at the beginning of an epoch.\n",
    "    # This learning rate will be used by the \"optimizer\" in the train_step function.\n",
    "    lr_epoch = optimizer._decayed_lr(tf.float32).numpy()\n",
    "    \n",
    "    total_loss = 0.0 # Stores the total loss during an epoch\n",
    "    num_batches = 0  # Stores the number of batches during the training in an epoch\n",
    "\n",
    "    # Get the start time for each epoch.\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Capture the training statistics via the comel ml object.\n",
    "    with experiment.train():\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (X_batch, y_batch) in enumerate(dist_train_loader):\n",
    "\n",
    "            '''\n",
    "            Compute the training loss per step.\n",
    "            The scaled loss is the return value of the distributed_train_step. \n",
    "            This value is aggregated across replicas using the tf.distribute.Strategy.reduce call,\n",
    "            and then across batches by summing the return value of the tf.distribute.Strategy.reduce calls.\n",
    "            '''\n",
    "            total_loss += distributed_train_step(X_batch, y_batch)\n",
    "\n",
    "            # Increment the step count.\n",
    "            iterations_total = iterations_total + 1\n",
    "            \n",
    "            # Increment the batch count within the current epoch\n",
    "            num_batches = num_batches + 1\n",
    "            \n",
    "            # Compute the training loss (up to the current batch).\n",
    "            train_loss = total_loss / num_batches\n",
    "        \n",
    "            # Display the training loss per step.\n",
    "            print_status_per_step(step * GLOBAL_BATCH_SIZE, len(y_train), train_loss)\n",
    "            \n",
    "        \n",
    "        # Compute the training accuracy at the end of each epoch.\n",
    "        train_acc = train_acc_epoch.result()\n",
    "\n",
    "        # Reset the training accuracy metric at the end of each epoch.\n",
    "        train_acc_epoch.reset_states()\n",
    "\n",
    "    # Capture the validation statistics via the comel ml object.\n",
    "    with experiment.test():\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for X_batch_val, y_batch_val in dist_val_loader:\n",
    "            distributed_val_step(X_batch_val, y_batch_val)\n",
    "\n",
    "        # Compute the mean val loss at the end of each epoch.\n",
    "        val_loss = val_loss_epoch.result()\n",
    "\n",
    "        # Compute the validation accuracy at the end of each epoch.\n",
    "        val_acc = val_acc_epoch.result()\n",
    "\n",
    "    # Compute the time taken for each epoch.\n",
    "    time_per_epoch = time.time() - start_time\n",
    "\n",
    "    # Display the training statistics at the end of each epoch.\n",
    "    print_status_per_epoch(len(y_train), len(y_train), train_loss, train_acc,\n",
    "                          val_loss, val_acc, lr_epoch, time_per_epoch)\n",
    "\n",
    "\n",
    "    # Reset validation metrics at the end of each epoch.\n",
    "    val_acc_epoch.reset_states()\n",
    "    val_loss_epoch.reset_states()\n",
    "\n",
    "    # Save checkpoints after every 10 epochs.\n",
    "    if epoch % 10 == 0:\n",
    "        # The epoch number is used to number the saved checkpoint.\n",
    "        manager.save(checkpoint_number=epoch)\n",
    "\n",
    "    # Define a set of metrics to be stored via the comet ml object.\n",
    "    metrics = {\n",
    "        'loss':train_loss,\n",
    "        'accuracy':train_acc,\n",
    "        'val_loss':val_loss,\n",
    "        'val_accuracy':val_acc,\n",
    "        'learning_rate':lr_epoch,\n",
    "        'epoch':epoch,\n",
    "        'iterations': iterations_total\n",
    "    }\n",
    "    # Log the metrics via the comet ml object.\n",
    "    experiment.log_metrics(metrics)\n",
    "\n",
    "# Log the following hyperparameters via the comet ml object.\n",
    "params={'batch_size':GLOBAL_BATCH_SIZE,\n",
    "        'epochs':no_of_epochs,\n",
    "        'iterations': iterations_total,\n",
    "        'optimizer':optimizer,\n",
    "        'scheduler': lr_schedule\n",
    "}\n",
    "experiment.log_parameters(params)\n",
    "experiment.end()\n",
    "\n",
    "print(\"\\n\\nTraining completed successfully! :)\\n\")    \n",
    "\n",
    "\n",
    "'''\n",
    "Save the final model to disk in the SavedModel format.\n",
    "SavedModel is a comprehensive save format that saves the model architecture, \n",
    "weights, and the traced Tensorflow subgraphs of the call functions. \n",
    "This enables Keras to restore both built-in layers as well as custom objects.\n",
    "'''\n",
    "\n",
    "print(\"\\nSaving the fully trained model in the SavedModel format ... \\n\\n\")\n",
    "\n",
    "# The model name variable is used for model serialization.\n",
    "final_model_name='Model-Distributed'\n",
    "\n",
    "# Path to the diretory in which the FINAL model will be serialized.\n",
    "final_model_directory='./Saved_Models/'\n",
    "\n",
    "# Path name of the final model.\n",
    "final_model_path = os.path.join(final_model_directory, final_model_name)\n",
    "\n",
    "\n",
    "'''\n",
    "Save the final model.\n",
    "\n",
    "When saving the model to a local I/O device while training on remote devices (e.g., GPUs on a remote node),\n",
    "we need to set the I/O device to localhost.\n",
    "This is done by using the option experimental_io_device in tf.saved_model.SaveOptions.\n",
    "'''\n",
    "model.save(final_model_path)\n",
    "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "model.save(final_model_path, options=save_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3807e3",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9991a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n###################### Model Evaluation using Test Data ######################\\n\\n\")\n",
    "\n",
    "# Predict test labels.\n",
    "y_test_pred = model(X_test, training=False)\n",
    "\n",
    "print(\"\\nTest Data Predictions (shape): \", y_test_pred.shape)\n",
    "print(\"Test Data (shape): \", y_test.shape)\n",
    "\n",
    "# Compute the test accuracy.\n",
    "acc_fn = tf.keras.metrics.CategoricalAccuracy()\n",
    "acc_fn.update_state(y_test, y_test_pred)\n",
    "test_accuracy = acc_fn.result().numpy()\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "# Compute the test loss.\n",
    "test_loss = compute_loss(y_test, y_test_pred, model.losses).numpy()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b1a40",
   "metadata": {},
   "source": [
    "## Model Evaluation (using the Saved Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b42db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n######################### Model Evaluation (using the Saved Model) #########################\\n\\n\")\n",
    "\n",
    "'''\n",
    "We evaluate the saved model using its predict() method on the test dataset.\n",
    "\n",
    "The saved model (that was stored in the SavedModel format) can be loaded \n",
    "by using tf.keras.models.load_model() method. \n",
    "\n",
    "NOTE: If the model contains custom layers, \n",
    "then we need to set the \"custom_objects\" argument of the \"load_model\" method. \n",
    "It should be a dictionary mapping names (strings) to custom classes or functions to be \n",
    "considered during deserialization.\n",
    "'''\n",
    "\n",
    "print(\"\\n\\nLoading the saved model...\\n\\n\")\n",
    "\n",
    "\n",
    "'''\n",
    "Load the final model.\n",
    "\n",
    "When loading the model from a local I/O device that was trained on remote devices (e.g., GPUs on a remote node),\n",
    "we need to set the I/O device to localhost.\n",
    "This is done by using the option experimental_io_device in tf.saved_model.LoadOptions.\n",
    "'''\n",
    "another_strategy = tf.distribute.MirroredStrategy()\n",
    "with another_strategy.scope():\n",
    "    load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "    saved_model = tf.keras.models.load_model(final_model_path, options=load_options,\n",
    "                                            custom_objects={\"vgg\": create_model})\n",
    "\n",
    "\n",
    "# Predict test labels (set the \"training\" argument false).\n",
    "y_test_pred = saved_model(X_test, training=False)\n",
    "\n",
    "print(\"\\nTest Data Predictions (shape): \", y_test_pred.shape)\n",
    "print(\"Test Data (shape): \", y_test.shape)\n",
    "\n",
    "# Compute the test accuracy.\n",
    "acc_fn = tf.keras.metrics.CategoricalAccuracy()\n",
    "acc_fn.update_state(y_test, y_test_pred)\n",
    "test_accuracy = acc_fn.result().numpy()\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "# Compute the test loss.\n",
    "test_loss = compute_loss(y_test, y_test_pred, model.losses).numpy()\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc6315",
   "metadata": {},
   "source": [
    "## Model Evaluation (using the Model Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef523a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nLoading the model checkpoint...\\n\")\n",
    "\n",
    "# Instantiate the model.\n",
    "layer_info = ((2, 64), (2, 128), (4, 256), (4, 512), (4, 512)) # for the VGGNet model\n",
    "new_model = create_model(layer_info, 32, 32, channels=3, num_classes=10,\n",
    "                           weight_decay=0.001)\n",
    "\n",
    "# Instantiate the Checkpoint and specify the model instance to be restored.\n",
    "checkpoint = tf.train.Checkpoint(model=new_model)\n",
    "\n",
    "\n",
    "'''\n",
    "Restore the parameter values of the model instance.\n",
    "\n",
    "NOTE: Use the function expect_partial() on the loaded status, \n",
    "since model saved from Keras often generates extra keys in the checkpoint. \n",
    "Otherwise, the program prints a lot of warnings about unused keys at exit time.\n",
    "'''\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_model_directory)).expect_partial()\n",
    "\n",
    "\n",
    "# Predict the test labels (set the \"training\" argument false) using the test NumPy test dataset.\n",
    "y_test_pred = new_model(X_test, training=False)\n",
    "\n",
    "\n",
    "# Compute the test accuracy.\n",
    "acc_fn = tf.keras.metrics.CategoricalAccuracy()\n",
    "acc_fn.update_state(y_test, y_test_pred)\n",
    "test_accuracy = acc_fn.result().numpy()\n",
    "print(\"\\nTest Accuracy (using NumPy test dataset): \", test_accuracy)\n",
    "\n",
    "# Compute the test loss.\n",
    "test_loss = compute_loss(y_test, y_test_pred, new_model.losses).numpy()\n",
    "print(\"Test Loss (using NumPy test dataset): \", test_loss)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "'''\n",
    "Alternatively, we can use the testDataset object to compute test accuracy & loss.\n",
    "First, we define a function for computing test accuracy & loss.\n",
    "'''\n",
    "\n",
    "@tf.function\n",
    "def test_step(X, y):\n",
    "    # Predict the labels.\n",
    "    y_pred = new_model(X, training=False)\n",
    "    # Compute accuracy.\n",
    "    accuracy = acc_fn(y, y_pred)\n",
    "    # Compute loss.\n",
    "    loss = compute_loss(y, y_pred, model.losses)\n",
    "    \n",
    "    # Return accuracy & loss\n",
    "    return accuracy, loss\n",
    "    \n",
    "\n",
    "'''\n",
    "Compute test accuracy & loss using the test Dataset object.\n",
    "'''\n",
    "for X, y in test_loader:\n",
    "    test_accuracy, test_loss = test_step(X, y)\n",
    "    \n",
    "    \n",
    "print(\"Test Accuracy (using test Dataset object): \", test_accuracy.numpy())\n",
    "print(\"Test Loss (using test Datsset object): \", test_loss.numpy())\n",
    "print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
